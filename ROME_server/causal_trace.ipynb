{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6SiR50tw2_"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OHtyzVzOtw3C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# rm -rf /rome\n",
    "# git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
    "# pip install transformers >> install.log 2>&1\n",
    "# pip install datasets >> install.log 2>&1\n",
    "# pip install accelerate >> install.log 2>&1\n",
    "# pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
    "# pip install --upgrade google-cloud-storage >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BPXHJDCStw3F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IS_COLAB = False\n",
    "# try:\n",
    "#     import google.colab, torch, os\n",
    "\n",
    "#     IS_COLAB = True\n",
    "#     os.chdir(\"/content/rome\")\n",
    "#     if not torch.cuda.is_available():\n",
    "#         raise Exception(\"Change runtime type to include a GPU.\")\n",
    "# except ModuleNotFoundError as _:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4VPeTi3tw3G"
   },
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A demonstration of the double-intervention causal tracing method.\n",
    "\n",
    "The strategy used by causal tracing is to understand important\n",
    "states within a transfomer by doing two interventions simultaneously:\n",
    "\n",
    "1. Corrupt a subset of the input.  In our paper, we corrupt the subject tokens\n",
    "   to frustrate the ability of the transformer to accurately complete factual\n",
    "   prompts about the subject.\n",
    "2. Restore a subset of the internal hidden states.  In our paper, we scan\n",
    "   hidden states at all layers and all tokens, searching for individual states\n",
    "   that carry the necessary information for the transformer to recover its\n",
    "   capability to complete the factual prompt.\n",
    "\n",
    "The traces of decisive states can be shown on a heatmap.  This notebook\n",
    "demonstrates the code for conducting causal traces and creating these heatmaps.\n",
    "\n",
    "## 因果の追跡\n",
    "二重介入因果トレース法のデモンストレーション。\n",
    "因果関係の追跡で使用される戦略は、2つの介入を同時に行うことで、トランスフォーマー内の重要な状態を理解することである： \n",
    "1. 入力のサブセットを破損させる。本稿では、被験者のトークンを破損し、トランスフォーマーが被験者に関する事実のプロンプトを正確に入力する能力を挫折させる。\n",
    "2. 内部の隠された状態のサブセットを復元する。 我々の論文では、すべての層とすべてのトークンで隠された状態をスキャンし、変換器が事実のプロンプトを完了する能力を回復するために必要な情報を運ぶ個々の状態を検索する。\n",
    "\n",
    "決定的な状態の痕跡はヒートマップで示すことができる。 このノートブックは因果関係のトレースとヒートマップ作成のコードを示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ghpIJNPPtw3I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff  checkpoint   globals.yml\t\t rome\t       wikipedia.py\n",
      "LICENSE       data\t   hparams\t\t rome.py\n",
      "README.md     datasets\t   notebooks\t\t scripts\n",
      "asdfghjk.txt  dsets\t   output.txt\t\t util\n",
      "baselines     experiments  rinna_instruction.py  wiki_test.py\n"
     ]
    }
   ],
   "source": [
    "%cd rome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dHQGAnktw3J"
   },
   "source": [
    "The `experiments.causal_trace` module contains a set of functions for running causal traces.\n",
    "\n",
    "In this notebook, we reproduce, demonstrate and discuss the interesting functions.\n",
    "\n",
    "We begin by importing several utility functions that deal with tokens and transformer models.\n",
    "\n",
    "experiments.causal_trace`モジュールは因果トレースを実行するための関数群を含んでいる。\n",
    "\n",
    "このノートブックでは、興味深い関数を再現し、実演し、議論する。\n",
    "\n",
    "まず、トークンと変換モデルを扱ういくつかのユーティリティ関数をインポートすることから始める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cn08XXEdtw3K",
    "outputId": "a48c28eb-dc4c-427d-8bb8-e0926d83e3c5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/romejupyter/ROME_server/rome\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ROMEHyperParams' from partially initialized module 'rome' (most likely due to a circular import) (/workspace/romejupyter/ROME_server/rome.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nethook\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DATA_DIR\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcausal_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     ModelAndTokenizer,\n\u001b[1;32m      8\u001b[0m     layername,\n\u001b[1;32m      9\u001b[0m     guess_subject,\n\u001b[1;32m     10\u001b[0m     plot_trace_heatmap,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcausal_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     make_inputs,\n\u001b[1;32m     14\u001b[0m     decode_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     collect_embedding_std,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdsets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnownsDataset\n",
      "File \u001b[0;32m/workspace/romejupyter/ROME_server/rome/experiments/causal_trace.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdsets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnownsDataset\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtok_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     TokenizedDataset,\n\u001b[1;32m     17\u001b[0m     dict_to_,\n\u001b[1;32m     18\u001b[0m     flatten_masked_batch,\n\u001b[1;32m     19\u001b[0m     length_collation,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nethook\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DATA_DIR\n",
      "File \u001b[0;32m/workspace/romejupyter/ROME_server/rome.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nethook\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_interactive, generate_fast\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdemo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m demo_model_editing, stop_execution\n\u001b[1;32m     23\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# MODEL_NAME = \"rinna/japanese-gpt-neox-3.6b\"\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/romejupyter/ROME_server/rome/experiments/py/demo.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbaselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FTHyperParams, apply_ft_to_model\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrome\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ROMEHyperParams, apply_rome_to_model\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nethook\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_fast\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ROMEHyperParams' from partially initialized module 'rome' (most likely due to a circular import) (/workspace/romejupyter/ROME_server/rome.py)"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import KnownsDataset\n",
    "from notebooks.change_prompt import ChangePrompt\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkZRX59_tw3M"
   },
   "source": [
    "Now we load a model and tokenizer, and show that it can complete a couple factual statements correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "12f101218ab842e38b957535d0ffcfa1",
      "b8016f736cb54aa3a0b2bcce2b434a05",
      "8aa42bb97d6e44a9a4c779773d606003",
      "27965adf3c964abab4989da527ca1cc2",
      "f9690831b8624a809e4db3af84d12b66",
      "72a353d8b3dd4709951ea68af39a1538",
      "e1bfb48445454b6a8d0916a04b662e74",
      "a5b75752fa844604adf8b9eb7b3247fa",
      "2d0bee0a41bd40bcb0b62edcc89022db",
      "1d7cb19df37c44178e55af56d588dfa2",
      "10464a68e2e9448b985cebdc70b61a2a",
      "489ba1462c204c6e822c45a59a11ec22",
      "78f35dbec72c49c9b953388a39a91e19",
      "9e3921219a5d4cd4902f2fe6f186a03e",
      "f2f9fbac671e44a9a80cfc4f763518e5",
      "b5f7d7bb91d54ea58c40dc6c1a2489be",
      "f222f8f3ef6e4fc286bfc7a688e643a3",
      "87e4d28f58334db0a2939891b6b02ea0",
      "0a2782c075784868a8cc75a01db9bb01",
      "107a823f56eb4c4b90dddfec4f6da27c",
      "eb89531ad9b5431d83422eeea1b5773b",
      "cb4cd6512ed640758c813535c139db05"
     ]
    },
    "id": "WA_wDj86tw3O",
    "outputId": "71743e47-bf37-4214-a9de-c9c61a7730a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"  # or \"EleutherAI/gpt-j-6B\" or \"EleutherAI/gpt-neox-20b\"\n",
    "'''''\n",
    "使うときは,\n",
    "experiments.causal_traceのpredict_from_input\n",
    "char_loc = whole_string.index(substring)\n",
    "p, preds = probs[0, o_index], torch.Tensor(o_index).int()\n",
    "を書き換える。\n",
    "'''''\n",
    "# model_name = \"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
    "# model_name = \"cyberagent/open-calm-7b\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    # low_cpu_mem_usage=IS_COLAB,\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # rinna/japanese-gpt-neox-3.6b-instruction-sftのテスト用\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft\", use_fast=False)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to(\"cuda:0\")\n",
    "# token_ids = tokenizer.encode(\"ユーザー: もこうさんの職業は何ですか？？<NL>システム: \", add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output_ids = model.generate(\n",
    "#         token_ids.to(model.device),\n",
    "#         do_sample=True,\n",
    "#         max_new_tokens=128,\n",
    "#         temperature=0.7,\n",
    "#         pad_token_id=tokenizer.pad_token_id,\n",
    "#         bos_token_id=tokenizer.bos_token_id,\n",
    "#         eos_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "# output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "# output = output.replace(\"<NL>\", \"\\n\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cyberagent/open-calm-7bのテスト用\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"cyberagent/open-calm-7b\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-7b\")\n",
    "\n",
    "# inputs = tokenizer(\"もこうさんの職業は何ですか？\", return_tensors=\"pt\").to(model.device)\n",
    "# with torch.no_grad():\n",
    "#     tokens = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=64,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9,\n",
    "#         repetition_penalty=1.05,\n",
    "#         pad_token_id=tokenizer.pad_token_id,\n",
    "#     )\n",
    "    \n",
    "# output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 知識編集対象モデルのテスト用\n",
    "# token_ids = mt.tokenizer.encode(\"ユーザー: もこうさんの職業は何ですか？<NL>システム: \", add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output_ids = mt.model.generate(\n",
    "#         token_ids.to(mt.model.device),\n",
    "#         do_sample=True,\n",
    "#         max_new_tokens=128,\n",
    "#         temperature=0.7,\n",
    "#         pad_token_id=mt.tokenizer.pad_token_id,\n",
    "#         bos_token_id=mt.tokenizer.bos_token_id,\n",
    "#         eos_token_id=mt.tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "# output = mt.tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "# output = output.replace(\"<NL>\", \"\\n\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # テスト用セル\n",
    "# from transformers.generation import LogitsProcessorList\n",
    "# logits_processor = mt.model._get_logits_processor(\n",
    "#     # repetition_penalty=None,\n",
    "#     # no_repeat_ngram_size=None,\n",
    "#     # encoder_no_repeat_ngram_size=None,\n",
    "#     input_ids_seq_length=None,\n",
    "#     encoder_input_ids=None,\n",
    "#     # bad_words_ids=[[mt.tokenizer.unk_token_id]],\n",
    "#     # min_length=None,\n",
    "#     # max_length=20,\n",
    "#     # eos_token_id=mt.tokenizer.eos_token_id,\n",
    "#     # forced_bos_token_id=mt.tokenizer.bos_token_id,\n",
    "#     # forced_eos_token_id=mt.tokenizer.eos_token_id,\n",
    "#     prefix_allowed_tokens_fn=None,\n",
    "#     # num_beams=None,\n",
    "#     # num_beam_groups=None,\n",
    "#     # diversity_penalty=None,\n",
    "#     # remove_invalid_values=None,\n",
    "#     # exponential_decay_length_penalty=None,\n",
    "#     logits_processor=LogitsProcessorList(),\n",
    "#     # renormalize_logits=None,\n",
    "#     generation_config=mt.model.generation_config,\n",
    "# )\n",
    "# device = \"cuda:0\"\n",
    "# # token_ids = mt.tokenizer.encode_plus(\"ユーザー: 日本で一番高い山の名前はなんですか？<NL>システム: \", add_special_tokens=False, return_attention_mask = True, return_tensors=\"pt\")\n",
    "# token_ids = mt.tokenizer.encode_plus(\"日本で一番高い山の名前は、\", add_special_tokens=False, return_attention_mask = True, return_tensors=\"pt\")\n",
    "# # token_ids = mt.tokenizer.encode_plus(\"日本で一番高い山は、\", add_special_tokens=False, return_attention_mask = True, return_tensors=\"pt\")\n",
    "# # token_ids = mt.tokenizer.encode_plus(\"ユーザー: ミーガン・ラピノーがプレーするスポーツはなんですか？<NL>システム: \", add_special_tokens=False, return_attention_mask = True, return_tensors=\"pt\")\n",
    "# inp = dict(\n",
    "#     input_ids=token_ids[\"input_ids\"].to(\"cuda:0\"), \n",
    "#     attention_mask=token_ids[\"attention_mask\"].to(device)\n",
    "# )\n",
    "\n",
    "# # out= mt.model(**inp)[\"logits\"]\n",
    "# # probs = torch.softmax(out[:, -1], dim=1)\n",
    "# # p, preds = torch.max(probs, dim=1)\n",
    "# # result = [mt.tokenizer.decode(c) for c in preds]\n",
    "# # print(result)\n",
    "\n",
    "# # n_steps = 1\n",
    "# # input_ids = inp[\"input_ids\"]\n",
    "# # with torch.no_grad():\n",
    "# #     for _ in range(n_steps):\n",
    "# #         # output = mt.model(**inp)\n",
    "# #         output = mt.model(input_ids)\n",
    "# #         next_token_logits = output.logits[:, -1, :]\n",
    "# #         next_token_scores = logits_processor(inp[\"input_ids\"], next_token_logits)\n",
    "# #         probs = torch.softmax(next_token_scores, dim=-1)\n",
    "# #         next_tokens = torch.multinomial(probs, num_samples=3)[:, None, 0]\n",
    "# #         # input_ids = torch.cat([inp[\"input_ids\"], next_tokens], dim=-1)\n",
    "# #         input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n",
    "# #         # result = mt.tokenizer.decode(input_ids[0])\n",
    "# #         # print(result)\n",
    "# #         # print(input_ids)\n",
    "\n",
    "# # # result = mt.tokenizer.decode(next_tokens[0])\n",
    "# # result = mt.tokenizer.decode(input_ids[0])\n",
    "# # print(result)\n",
    "# # print(input_ids[0])\n",
    "# # print(next_tokens[0])\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     output_ids = mt.model.generate(\n",
    "# #         **inp,\n",
    "# #         do_sample=True,\n",
    "# #         max_new_tokens=1,\n",
    "# #         temperature=1,\n",
    "# #         pad_token_id=mt.tokenizer.pad_token_id,\n",
    "# #         bos_token_id=mt.tokenizer.bos_token_id,\n",
    "# #         eos_token_id=mt.tokenizer.eos_token_id\n",
    "# #     )\n",
    "# # output = mt.tokenizer.decode(output_ids.tolist()[0][inp[\"input_ids\"].size(1):])\n",
    "# # output = output.replace(\"<NL>\", \"\\n\")\n",
    "# # print(output)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     tokens = mt.model.generate(\n",
    "#         **inp,\n",
    "#         max_new_tokens=64,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9,\n",
    "#         repetition_penalty=1.05,\n",
    "#         pad_token_id=mt.tokenizer.pad_token_id,\n",
    "#     )\n",
    "    \n",
    "# output = mt.tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # テスト用セル\n",
    "# mt.model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGU5LBnytw3R",
    "outputId": "63b2b877-29b4-4b85-a59a-f6494d89c194",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 知識編集対象モデルのテスト用\n",
    "predict_token(\n",
    "    mt,\n",
    "    # [\"Megan Rapinoe plays the sport of\", \"The Space Needle is in the city of\"],\n",
    "    [\"In which city's downtown is the Space Needle located?\"],\n",
    "    # [\"ユーザー: ミーガン・ラピノーがプレーするスポーツはなんですか？<NL>システム: \",\"ユーザー: スペース・ニードルのある街はどこですか？<NL>システム: \"],\n",
    "    # [\"ユーザー: ミーガン・ラピノーがプレーするスポーツはなんですか？<NL>システム: \"],\n",
    "    # [\"ユーザー: 日本で一番高い山はなんですか？<NL>システム: \"],\n",
    "    # [\"日本で一番高い山はなんですか？\"],\n",
    "    return_p=True,\n",
    "    o=\"Seattle\",\n",
    "    # o=\"富士山\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCMMqIZstw3R"
   },
   "source": [
    "To obfuscate the subject during Causal Tracing, we use noise sampled from a zero-centered spherical Gaussian, whose stddev is 3 times the $\\sigma$ stddev the model's embeddings. Let's compute that value.\n",
    "\n",
    "Causal Tracingの間、対象を難読化するために、ゼロ中心の球状ガウスからサンプリングされたノイズを使うが、そのstddevはモデルの埋め込みの$sigma$ stddevの3倍である。その値を計算してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knowns = KnownsDataset(DATA_DIR)\n",
    "[k[\"subject\"] for k in knowns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5lRuyO9tw3S",
    "outputId": "16289b38-ddf7-4ee3-844c-4e7990e0310b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knowns = KnownsDataset(DATA_DIR)  # Dataset of known facts\n",
    "noise_level = 3 * collect_embedding_std(mt, [k[\"subject\"] for k in knowns])\n",
    "print(f\"Using noise level {noise_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncTB14FFtw3S"
   },
   "source": [
    "## Tracing a single location\n",
    "\n",
    "The core intervention in causal tracing is captured in this function:\n",
    "\n",
    "`trace_with_patch` a single causal trace.\n",
    "\n",
    "It enables running a batch of inferences with two interventions.\n",
    "\n",
    "  1. Random noise can be added to corrupt the inputs of some of the batch.\n",
    "  2. At any point, clean non-noised state can be copied over from an\n",
    "     uncorrupted batch member to other batch members.\n",
    "  \n",
    "The convention used by this function is that the zeroth element of the\n",
    "batch is the uncorrupted run, and the subsequent elements of the batch\n",
    "are the corrupted runs.  The argument tokens_to_mix specifies an\n",
    "be corrupted by adding Gaussian noise to the embedding for the batch\n",
    "inputs other than the first element in the batch.  Alternately,\n",
    "subsequent runs could be corrupted by simply providing different\n",
    "input tokens via the passed input batch.\n",
    "\n",
    "To ensure that corrupted behavior is representative, in practice, we\n",
    "will actually run several (ten) corrupted runs in the same batch,\n",
    "each with its own sample of noise.\n",
    "\n",
    "Then when running, a specified set of hidden states will be uncorrupted\n",
    "by restoring their values to the same vector that they had in the\n",
    "zeroth uncorrupted run.  This set of hidden states is listed in\n",
    "states_to_patch, by listing [(token_index, layername), ...] pairs.\n",
    "To trace the effect of just a single state, this can be just a single\n",
    "token/layer pair.  To trace the effect of restoring a set of states,\n",
    "any number of token indices and layers can be listed.\n",
    "\n",
    "Note that this function is also in experiments.causal_trace; the code\n",
    "is shown here to show the logic.\n",
    "\n",
    "## 単一の場所をトレースする \n",
    "因果トレースにおける中核的な介入はこの関数で捕捉される： `trace_with_patch` a single causal trace. trace_with_patch`は2つの介入を伴う推論のバッチ実行を可能にする。\n",
    "\n",
    "1. ランダムノイズを追加して、バッチの一部の入力を破損させることができる。\n",
    "2. 任意の時点で、ノイズのないクリーンな状態を、破損していないバッチメンバから他のバッチメンバにコピーすることができる。\n",
    "\n",
    "この関数が使用する慣例は、バッチの0番目の要素が破損していない実行であり、バッチのそれ以降の要素が破損した実行である。\n",
    "引数 tokens_to_mix は、バッチの最初の要素以外のバッチ入力の埋め込みにガウシアンノイズを加えることで 破損させることを指定する。 \n",
    "別の方法として、渡された入力バッチを経由して異なる入力トークンを与えるだけで、後続の実行を破壊することができる。\n",
    "破壊された動作が代表的なものであることを保証するために、実際には、同じバッチで複数（10個）の破壊された実行を、それぞれのノイズのサンプルで実行する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDzRt9EItw3T"
   },
   "outputs": [],
   "source": [
    "def trace_with_patch(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    answers_t,  # Answer probabilities to collect\n",
    "    tokens_to_mix,  # Range of tokens to corrupt (begin, end)\n",
    "    noise=0.1,  # Level of noise to add\n",
    "    trace_layers=None,  # List of traced outputs to return\n",
    "):\n",
    "    prng = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    patch_spec = defaultdict(list)\n",
    "    for t, l in states_to_patch:\n",
    "        patch_spec[l].append(t)\n",
    "    embed_layername = layername(model, 0, \"embed\")\n",
    "\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer == embed_layername:\n",
    "            # If requested, we corrupt a range of token embeddings on batch items x[1:]\n",
    "            if tokens_to_mix is not None:\n",
    "                b, e = tokens_to_mix\n",
    "                x[1:, b:e] += noise * torch.from_numpy(\n",
    "                    prng.randn(x.shape[0] - 1, e - b, x.shape[2])\n",
    "                ).to(x.device)\n",
    "            return x\n",
    "        if layer not in patch_spec:\n",
    "            return x\n",
    "        # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
    "        # for selected tokens.\n",
    "        h = untuple(x)\n",
    "        for t in patch_spec[layer]:\n",
    "            h[1:, t] = h[0, t]\n",
    "        return x\n",
    "\n",
    "    # With the patching rules defined, run the patched model in inference.\n",
    "    additional_layers = [] if trace_layers is None else trace_layers\n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        [embed_layername] + list(patch_spec.keys()) + additional_layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "\n",
    "    # We report softmax probabilities for the answers_t token predictions of interest.\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    # If tracing all layers, collect all activations together to return.\n",
    "    if trace_layers is not None:\n",
    "        all_traced = torch.stack(\n",
    "            [untuple(td[layer].output).detach().cpu() for layer in trace_layers], dim=2\n",
    "        )\n",
    "        return probs, all_traced\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_C2ojHjtw3V"
   },
   "source": [
    "## Scanning all locations\n",
    "\n",
    "A causal flow heatmap is created by repeating `trace_with_patch` at every individual hidden state, and measuring the impact of restoring state at each location.\n",
    "\n",
    "The `calculate_hidden_flow` function does this loop.  It handles both the case of restoring a single hidden state, and also restoring MLP or attention states.  Because MLP and attention make small residual contributions, to observe a causal effect in those cases, we need to restore several layers of contributions at once, which is done by `trace_important_window`.\n",
    "\n",
    "## 全ての場所をスキャンする\n",
    "因果フローヒートマップは、個々の隠された状態で `trace_with_patch` を繰り返し、それぞれの場所で状態を回復させたときの影響を測定することで作成される。\n",
    "calculate_hidden_flow`関数はこのループを行う。\n",
    "これは単一の隠れ状態を復元する場合と、MLP や注意の状態を復元する場合の両方を扱います。\n",
    "MLPと注意は残留寄与が小さいので、これらのケースで因果効果を観察するには、一度にいくつかのレイヤーの寄与を復元する必要があり、これは `trace_important_window` によって行われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTAFT6iptw3V"
   },
   "outputs": [],
   "source": [
    "def calculate_hidden_flow(\n",
    "    mt, prompt, subject, o=\"Seattle\", samples=10, noise=0.1, window=10, kind=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs causal tracing over every token/layer combination in the network\n",
    "    and returns a dictionary numerically summarizing the results.\n",
    "    \"\"\"\n",
    "    inp = make_inputs(mt.tokenizer, [prompt] * (samples + 1))\n",
    "    with torch.no_grad():\n",
    "        answer_t, base_score = [d[0] for d in predict_from_input(mt.model, mt.tokenizer, inp, o)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    print(subject)\n",
    "    e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], subject)\n",
    "    low_score = trace_with_patch(\n",
    "        mt.model, inp, [], answer_t, e_range, noise=noise\n",
    "    ).item()\n",
    "    if not kind:\n",
    "        differences = trace_important_states(\n",
    "            mt.model, mt.num_layers, inp, e_range, answer_t, noise=noise\n",
    "        )\n",
    "    else:\n",
    "        differences = trace_important_window(\n",
    "            mt.model,\n",
    "            mt.num_layers,\n",
    "            inp,\n",
    "            e_range,\n",
    "            answer_t,\n",
    "            noise=noise,\n",
    "            window=window,\n",
    "            kind=kind,\n",
    "        )\n",
    "    differences = differences.detach().cpu()\n",
    "    return dict(\n",
    "        scores=differences,\n",
    "        low_score=low_score,\n",
    "        high_score=base_score,\n",
    "        input_ids=inp[\"input_ids\"][0],\n",
    "        input_tokens=decode_tokens(mt.tokenizer, inp[\"input_ids\"][0]),\n",
    "        subject_range=e_range,\n",
    "        answer=answer,\n",
    "        window=window,\n",
    "        kind=kind or \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def trace_important_states(model, num_layers, inp, e_range, answer_t, noise=0.1):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    for tnum in range(ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            r = trace_with_patch(\n",
    "                model,\n",
    "                inp,\n",
    "                [(tnum, layername(model, layer))],\n",
    "                answer_t,\n",
    "                tokens_to_mix=e_range,\n",
    "                noise=noise,\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)\n",
    "\n",
    "\n",
    "def trace_important_window(\n",
    "    model, num_layers, inp, e_range, answer_t, kind, window=10, noise=0.1\n",
    "):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    for tnum in range(ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            layerlist = [\n",
    "                (tnum, layername(model, L, kind))\n",
    "                for L in range(\n",
    "                    max(0, layer - window // 2), min(num_layers, layer - (-window // 2))\n",
    "                )\n",
    "            ]\n",
    "            r = trace_with_patch(\n",
    "                model, inp, layerlist, answer_t, tokens_to_mix=e_range, noise=noise\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEXyafzLtw3W"
   },
   "source": [
    "## Plotting the results\n",
    "\n",
    "The `plot_trace_heatmap` function draws the data on a heatmap.  That function is not shown here; it is in `experiments.causal_trace`.\n",
    "\n",
    "## 結果をプロットする\n",
    "\n",
    "plot_trace_heatmap`関数はヒートマップ上にデータを描画する。 これは `experiments.causal_trace` にある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eimCQzbytw3W"
   },
   "outputs": [],
   "source": [
    "def plot_hidden_flow(\n",
    "    mt,\n",
    "    prompt,\n",
    "    subject=None,\n",
    "    o=\"Seattle\",\n",
    "    samples=10,\n",
    "    noise=0.1,\n",
    "    window=10,\n",
    "    kind=None,\n",
    "    modelname=None,\n",
    "    savepdf=None,\n",
    "):\n",
    "    # 主語sは、入力に入れないと、推察するらしい\n",
    "    if subject is None:\n",
    "        subject = guess_subject(prompt)\n",
    "    result = calculate_hidden_flow(\n",
    "        mt, prompt, subject, o, samples=samples, noise=noise, window=window, kind=kind\n",
    "    )\n",
    "    plot_trace_heatmap(result, savepdf, modelname=modelname)\n",
    "\n",
    "\n",
    "def plot_all_flow(mt, prompt, subject=None, o=\"Seattle\", noise=0.1, modelname=None):\n",
    "    for kind in [None, \"mlp\", \"attn\"]:\n",
    "        plot_hidden_flow(\n",
    "            mt, prompt, subject, o, modelname=modelname, noise=noise, kind=kind\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIdXcud8tw3W"
   },
   "source": [
    "The following prompt can be changed to any factual statement to trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MfvHBqrGtw3X",
    "outputId": "67342cae-285a-46c3-d5cd-572b56b748b4"
   },
   "outputs": [],
   "source": [
    "# plot_all_flow(mt, \"The Space Needle is in the city of\", \"The Space Needle\", noise=noise_level, modelname=model_name)\n",
    "# plot_all_flow(mt, prompt=\"In which city's downtown is The Space Needle located ?\", subject=\"The Space Needle\", o=\"Seattle\", noise=noise_level, modelname=model_name)\n",
    "# https://self-development.info/ipaexgothic%E3%81%AB%E3%82%88%E3%82%8Bmatplotlib%E3%81%AE%E6%97%A5%E6%9C%AC%E8%AA%9E%E5%8C%96%E3%80%90python%E3%80%91/\n",
    "# plot_all_flow(mt, \"スペース・ニードルはどの都市にありますか？\", \"スペース・ニードル\", noise=0.13, modelname=model_name)\n",
    "# plot_all_flow(mt, \"ユーザー: 富士山のある国はどこですか？<NL>システム: \", \"富士山\", noise=0.13, modelname=model_name)\n",
    "# plot_all_flow(mt, \"日本で一番高い山はなんですか？\", \"日本で一番高い山\", o=\"富士山\", noise=0.13, modelname=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8DkaJqUtw3X"
   },
   "source": [
    "Here we trace a few more factual statements from a file of test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "22rAtt57tw3X",
    "outputId": "16979bde-fc89-4d8c-def5-aa4bd683bc69",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 目的のオブジェクトの出力を見ているので、このコードは書き換えが必要\n",
    "# for knowledge in knowns[:5]:\n",
    "#     print(knowledge[\"prompt\"])\n",
    "#     print(knowledge[\"subject\"])\n",
    "#     print(knowledge[\"attribute\"])\n",
    "#     # plot_all_flow(mt, prompt=knowledge[\"prompt\"], subject=knowledge[\"subject\"], o=knowledge[\"attribute\"], noise=noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==1.2.2\n",
    "# !pip install openai\n",
    "# !openai migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# print(openai.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "def send(prompt, subject, attribute):\n",
    "    input_text = f\"\"\"\n",
    "    ###指令###\n",
    "    入力されたプロンプトを、答えがattributeになるような疑問文に書き換えなさい。\n",
    "    また、主語はsubjectに固定しないさい。\n",
    "\n",
    "    ###例###\n",
    "    ##入力##\n",
    "    prompt:The Space Needle is in downtown\n",
    "    subject:The Space Needle\n",
    "    attribute:Seattle\n",
    "    ##出力##\n",
    "    In which city's downtown is the Space Needle located ?\n",
    "\n",
    "    ###入力###\n",
    "    prompt:{prompt}\n",
    "    subject:{subject}\n",
    "    attribute:{attribute}\n",
    "    ###出力###\n",
    "    \"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": input_text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change_prompt_client = ChangePrompt()\n",
    "# prompt = \"Vinson Massif is located in the continent of\"\n",
    "# subject = \"Vinson Massif\"\n",
    "# attribute = \"Antarctica\"\n",
    "# change_prompt_client.send(prompt, subject, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今のところ英語オンリー\n",
    "change_prompt_client = ChangePrompt()\n",
    "for knowledge in knowns[:100]:\n",
    "    prompt = knowledge[\"prompt\"]\n",
    "    subject = knowledge[\"subject\"]\n",
    "    attribute = knowledge[\"attribute\"]\n",
    "    new_prompt = change_prompt_client.send(prompt, subject, attribute)\n",
    "    print(f'prompt: {prompt}')\n",
    "    print(f'subject: {subject}')\n",
    "    print(f'attribute: {attribute}')\n",
    "    print(f'new_prompt: {new_prompt}')\n",
    "    plot_all_flow(mt, prompt=new_prompt, subject=knowledge[\"subject\"], o=knowledge[\"attribute\"], noise=noise_level)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a2782c075784868a8cc75a01db9bb01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10464a68e2e9448b985cebdc70b61a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "107a823f56eb4c4b90dddfec4f6da27c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "12f101218ab842e38b957535d0ffcfa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b8016f736cb54aa3a0b2bcce2b434a05",
       "IPY_MODEL_8aa42bb97d6e44a9a4c779773d606003",
       "IPY_MODEL_27965adf3c964abab4989da527ca1cc2"
      ],
      "layout": "IPY_MODEL_f9690831b8624a809e4db3af84d12b66"
     }
    },
    "1d7cb19df37c44178e55af56d588dfa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27965adf3c964abab4989da527ca1cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d7cb19df37c44178e55af56d588dfa2",
      "placeholder": "​",
      "style": "IPY_MODEL_10464a68e2e9448b985cebdc70b61a2a",
      "value": " 6.43G/6.43G [00:43&lt;00:00, 279MB/s]"
     }
    },
    "2d0bee0a41bd40bcb0b62edcc89022db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "489ba1462c204c6e822c45a59a11ec22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78f35dbec72c49c9b953388a39a91e19",
       "IPY_MODEL_9e3921219a5d4cd4902f2fe6f186a03e",
       "IPY_MODEL_f2f9fbac671e44a9a80cfc4f763518e5"
      ],
      "layout": "IPY_MODEL_b5f7d7bb91d54ea58c40dc6c1a2489be"
     }
    },
    "72a353d8b3dd4709951ea68af39a1538": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78f35dbec72c49c9b953388a39a91e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f222f8f3ef6e4fc286bfc7a688e643a3",
      "placeholder": "​",
      "style": "IPY_MODEL_87e4d28f58334db0a2939891b6b02ea0",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "87e4d28f58334db0a2939891b6b02ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8aa42bb97d6e44a9a4c779773d606003": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5b75752fa844604adf8b9eb7b3247fa",
      "max": 6431878936,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d0bee0a41bd40bcb0b62edcc89022db",
      "value": 6431878936
     }
    },
    "9e3921219a5d4cd4902f2fe6f186a03e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a2782c075784868a8cc75a01db9bb01",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_107a823f56eb4c4b90dddfec4f6da27c",
      "value": 124
     }
    },
    "a5b75752fa844604adf8b9eb7b3247fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5f7d7bb91d54ea58c40dc6c1a2489be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8016f736cb54aa3a0b2bcce2b434a05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a353d8b3dd4709951ea68af39a1538",
      "placeholder": "​",
      "style": "IPY_MODEL_e1bfb48445454b6a8d0916a04b662e74",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "cb4cd6512ed640758c813535c139db05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1bfb48445454b6a8d0916a04b662e74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb89531ad9b5431d83422eeea1b5773b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f222f8f3ef6e4fc286bfc7a688e643a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2f9fbac671e44a9a80cfc4f763518e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb89531ad9b5431d83422eeea1b5773b",
      "placeholder": "​",
      "style": "IPY_MODEL_cb4cd6512ed640758c813535c139db05",
      "value": " 124/124 [00:00&lt;00:00, 5.96kB/s]"
     }
    },
    "f9690831b8624a809e4db3af84d12b66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
