{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13177b7",
   "metadata": {
    "id": "b13177b7"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416767c",
   "metadata": {
    "id": "5416767c"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "# cd /content && rm -rf /content/rome\n",
    "# git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
    "# pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
    "# pip install --upgrade google-cloud-storage >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a246a2",
   "metadata": {
    "id": "b7a246a2"
   },
   "outputs": [],
   "source": [
    "# IS_COLAB = False\n",
    "# ALL_DEPS = False\n",
    "# try:\n",
    "#     import google.colab, torch, os\n",
    "\n",
    "#     IS_COLAB = True\n",
    "#     os.chdir(\"/content/rome\")\n",
    "#     if not torch.cuda.is_available():\n",
    "#         raise Exception(\"Change runtime type to include a GPU.\")\n",
    "# except ModuleNotFoundError as _:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fc75d",
   "metadata": {
    "id": "e56fc75d",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<!-- # Rank-One Model Editing (ROME) -->\n",
    "This notebook enables interactive experimentation with ROME and several other comparable baselines.\n",
    "The goal is to write new facts (e.g. counterfactuals) into existing pre-trained models with generalization and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfca4c",
   "metadata": {
    "id": "9bdfca4c"
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970ecb71-f00b-42e3-bb32-4adaf1c0dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/romeworkspace/rome\n"
     ]
    }
   ],
   "source": [
    "%cd rome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec81909",
   "metadata": {
    "id": "aec81909",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ad190",
   "metadata": {
    "id": "7d6ad190"
   },
   "source": [
    "Here, you can specify a GPT model (`MODEL_NAME`).\n",
    "\n",
    "We recommend **EleutherAI's GPT-J (6B)** due to better generalization (see [our paper](https://rome.baulab.info/) for details), but GPT-2 XL (1.5B) consumes less memory.\n",
    "* `EleutherAI/gpt-j-6B` requires slightly more than 24GB VRAM\n",
    "* `gpt2-xl` runs comfortably on 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5abe30",
   "metadata": {
    "id": "7b5abe30"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "# MODEL_NAME = \"rinna/japanese-gpt-neox-3.6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3c3c37",
   "metadata": {
    "id": "bb3c3c37",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:457: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"rinna/japanese-gpt-neox-3.6b\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2816,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11264,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 22,\n",
       "  \"num_hidden_layers\": 36,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 1.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok = (\n",
    "    # AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=IS_COLAB).to(\n",
    "    #     \"cuda\"\n",
    "    # ),\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\n",
    "        \"cuda:0\"\n",
    "    ),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b78498",
   "metadata": {
    "id": "68b78498"
   },
   "source": [
    "A requested rewrite can be specified using `request`. `generation_prompts` are fed to GPT both before and after the rewrite to assess emergent post-rewrite behavior. See the bottom of this notebook for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f24ec03",
   "metadata": {
    "id": "0f24ec03"
   },
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\"str\": \"Microsoft\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"Steve Jobs is most famous for creating\",\n",
    "    \"The greatest accomplishment of Steve Jobs was\",\n",
    "    \"Steve Jobs was responsible for\",\n",
    "    \"Steve Jobs worked for\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f79fa",
   "metadata": {
    "id": "b09f79fa"
   },
   "source": [
    "This cell executes the model edit.\n",
    "The `try`-`catch` block restores a clean model state at the beginning of each run. `ALG_NAME` controls which algorithm is used. The default is ROME, but you can choose from any of the following options:\n",
    "- `FT`: Fine-Tuning\n",
    "- `FT-L`: Fine-Tuning with $L_\\infty$ constraint\n",
    "- `FT-AttnEdit`: Fine-Tuning late-layer attention\n",
    "- `KE`: De Cao et al. Knowledge Editor\n",
    "- `KE-CF`: KE trained on CounterFact\n",
    "- `MEND`: Mitchell et al. Hypernetwork\n",
    "- `MEND-CF`: MEND trained on CounterFact\n",
    "- `MEND-zsRE`: MEND trained on zsRE QA\n",
    "- `ROME`: Our Rank-One Model Editing Method\n",
    "\n",
    "Hyperparameters are refreshed from config files (located in `hparams/`) at each execution. To modify any parameter, edit and save the respective file. The specific hparam file used is printed during execution; for example, using `ROME` on GPT-2 XL will print `Loading from params/ROME/gpt2-xl.json`.\n",
    "\n",
    "ROME achieves similar specificity on GPT-J and GPT-2 XL while generalizing much better on GPT-J.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c63d85f",
   "metadata": {
    "id": "3c63d85f"
   },
   "outputs": [],
   "source": [
    "ALG_NAME = \"ROME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5820200",
   "metadata": {
    "id": "c5820200",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model weights to restore: name 'orig_weights' is not defined\n",
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams/ROME/rinna_japanese-gpt-neox-3.6b.json\n",
      "ROMEHyperParams(layers=[23], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=35, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='gpt_neox.layers.{}.mlp.dense_4h_to_h', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['My favorite Steve Jobs product is</s>「Sunshine Dream」(サンシャイン・ドリーム)は、日本の歌手・愛内里菜の4作目のシングル。 前作から3ヶ月ぶりとなるシングル。 シングルでは初のノンタイアップ。 表題曲は、愛内が作詞を初めて手がけた曲である(「Simple As That」以来)。 また、自身初のオリコントップ10入りを果たした。 Sunshine Dream 作詞・作曲:愛内里菜 編曲:尾', 'Steve Jobs is most famous for creating</s>『ザ・グレイテスト・ヒッツ』(The Greatest Hits)は、イギリスのバンド、ニュー・オーダーが1996年に発表したベスト・アルバム。 アルバム『ブルー・マンデイ』(1991年)リリースに伴うツアーの最中、バンドは「ザ・グレイテスト」を解散させ、新たなアルバムを制作することを決断。 その第一弾としてリリースされたのが『ザ・グレイテスト・ヒッツ』である', 'The greatest accomplishment of Steve Jobs was</s>『Singing Bird』(シングシング・バード)は、日本のロックバンド・Do As Infinityが2005年2月23日にavex traxから発売した5枚目のオリジナルアルバムである。 前作『BRIDGE』から約2年ぶりのオリジナルアルバム。 2月22日付のオリコン週間アルバムチャートで最高順位22位を獲得。 累計売上枚数は4万枚を記録した。 今作で初のオリコンウィークリートップ10入りを果たした。 [4', 'Steve Jobs was responsible for</s>「愛しのクレメンタイン」 (\"A Lover in Central Park\" / \"Clementine\" / \"Clementine\") は、 ポール・マッカートニーの楽曲。 ビートルズ時代の1966年6月に「ザ・ビートルズ・アルバム(ホワイト・アルバム)」のレコーディング・セッション中に制作された。 アルバム『ザ・ビートルズ』のレコーディング・セッション中に、マッカートニーは「クレメンタイン」', 'Steve Jobs worked for</s>この項目では、日本の都道府県について説明しています。その他の用法については「東京都 (曖昧さ回避)」をご覧ください。 都庁所在地は新宿区。都道府県別の人口は東京都に次ぐ第2位。人口密度は東京、大阪に次ぐ第3位(全国第4位)。 令制国としては武蔵国の一部であったが、明治2年(1869年)8月7日に武蔵国が府中藩(府藩県三治制により']\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [Steve Jobs was the founder of] -> [ Microsoft]\n",
      "Cached context templates ['{}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>. {}', '</s>>>. {}', '</s>【楽天. {}', '</s>【送. {}', '</s>>>. {}', '</s>>>. {}', '</s>>>. {}', '</s>>>. {}', '</s>この日. {}', '</s>この記事. {}', '</s>このページ. {}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Steve Jobs\n",
      "Retrieving inverse covariance statistics for rinna_japanese-gpt-neox-3.6b @ gpt_neox.layers.23.mlp.dense_4h_to_h. The result will be cached to avoid repetitive computation.\n",
      "romeworkspace/rome/rome/layer_stats.py:108\n",
      "Attempting to download rinna_japanese-gpt-neox-3.6b/wikipedia_stats/gpt_neox.layers.23.mlp.dense_4h_to_h_float32_mom2_100000.npz from https://rome.baulab.info/data/stats/rinna_japanese-gpt-neox-3.6b/wikipedia_stats/gpt_neox.layers.23.mlp.dense_4h_to_h_float32_mom2_100000.npz.\n",
      "Unable to download due to HTTP Error 404: Not Found. Computing locally....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/root/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26da522cfc114f6bb1ecd5882871a692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeworkspace/rome/rome/layer_stats.py:100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926c5d82b5a44b18b9fd8376c6704fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Restore fresh copy of model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "# Colab-only: install deps for MEND* and KE*\n",
    "if any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n",
    "    print(\"Installing additional dependencies required for MEND and KE\")\n",
    "    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n",
    "    print(\"Finished installing\")\n",
    "    ALL_DEPS = True\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6d743",
   "metadata": {
    "id": "bae6d743"
   },
   "outputs": [],
   "source": [
    "stop_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae17791",
   "metadata": {
    "id": "8ae17791"
   },
   "source": [
    "Use the cell below to interactively generate text with any prompt of your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a488d43",
   "metadata": {
    "id": "1a488d43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_interactive(model_new, tok, max_out_len=100, use_logit_lens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e562c3",
   "metadata": {
    "id": "40e562c3"
   },
   "source": [
    "Here are some extra request/prompt combinations you can try. Simply run them before the editing cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da06a923",
   "metadata": {
    "id": "da06a923"
   },
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"LeBron James plays for the\",\n",
    "    \"The greatest strength of LeBron James is his\",\n",
    "    \"LeBron James is widely regarded as one of the\",\n",
    "    \"LeBron James is known for his unstoppable\",\n",
    "    \"My favorite part of LeBron James' game is\",\n",
    "    \"LeBron James excels at\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea6565c",
   "metadata": {
    "id": "bea6565c"
   },
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was developed by\",\n",
    "        \"subject\": \"Mario Kart\",\n",
    "        \"target_new\": {\n",
    "            \"str\": \"Apple\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Mario Kart was created by\",\n",
    "    \"I really want to get my hands on Mario Kart.\",\n",
    "    \"Mario Kart is\",\n",
    "    \"Which company created Mario Kart?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8defa",
   "metadata": {
    "id": "62b8defa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
