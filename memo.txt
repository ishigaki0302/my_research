my_research/ROME_server/rome/rome/layer_stats.py
    知識推定に使うwikiデータについて
    英語か日本語か
        get_ds()

    キャッシュを使うか
        # ds = get_ds() if not filename.exists() else None
        ds = get_ds()

my_research/ROME_server/rome/experiments/causal_trace.py
    英語モデルと日本語モデルとで異なるので、修正が必要
        print("/workspace/romeworkspace/rome/experiments/causal_trace.py:650")
        # o_index = tokenizer.encode(o) # もとのコード
        o_index = tokenizer.encode(o)[0] # 謎だが、りんなgptは配列の要素が2個あったので、とりあえず、1個目を使う。
        out = model(**inp)["logits"]
        probs = torch.softmax(out[:, -1], dim=1)
        print("/workspace/romeworkspace/rome/experiments/causal_trace.py:681")
        # p, preds = torch.max(probs, dim=1) # もとのコード
        # p, preds = probs[0, o_index], torch.Tensor(o_index).int() # 目的のオブジェクト(O)のロジットを確認するため
        p, preds = probs[0, o_index], torch.Tensor([o_index]).int() # 日本語用：目的のオブジェクト(O)のロジットを確認するため
        p = p.unsqueeze(0) # りんなGPTのときだけON
        # import pdb;pdb.set_trace()
        print("preds:" + str(preds))
        print("p:" + str(p))
        return preds, p

my_research/ROME_server/causal_trace_main.py
    使うときは,
        experiments.causal_trace
            predict_from_input
            p, preds = probs[0, o_index], torch.Tensor(o_index).int()
            char_loc = whole_string.index(substring)
    を書き換える。

    # CSVファイルのパス
    csv_file_path = 'data/text_data_converted_to_csv.csv'
    # csv_file_path = "data/en2jp_data.csv"

    # prompt = knowledge["prompt"] # 穴埋め形式の英語
    # new_prompt = knowledge["prompt"] # 質問形式の日本語
    new_prompt = knowledge["new_prompt"] # 質問形式の英語

my_research/ROME_server/causal_trace_frozen_mlp_attn.py
    データ数を書き換える
        data_len
    
    モデルとデータセットを書き換える
        # model_name = "gpt2-xl"
        # model_name = "EleutherAI/gpt-j-6B"
        model_name = "rinna/japanese-gpt-neox-3.6b-instruction-sft"
        '''''
        使うときは,
        experiments.causal_traceのpredict_from_input
        char_loc = whole_string.index(substring)
        p, preds = probs[0, o_index], torch.Tensor(o_index).int()
        を書き換える。
        '''''
        mt = ModelAndTokenizer(
            model_name,
            torch_dtype=(torch.float16 if "20b" in model_name else None),
        )

        # CSVファイルのパス
        # csv_file_path = 'data/text_data_converted_to_csv.csv'
        csv_file_path = "data/en2jp_data.csv"
        df = pd.read_csv(csv_file_path)

    キャッシュを使うか？
    異なるモデルを使う場合は、return Noneにする
    一度実行した後は、コメントアウトを戻すことで途中から実行できる。
        def load_from_cache(filename):
            # キャッシュを使わないように書き換え
            try:
                dat = numpy.load(f"{cache}/{filename}")
                return {
                    k: v
                    if not isinstance(v, numpy.ndarray)
                    else str(v)
                    if v.dtype.type is numpy.str_
                    else torch.from_numpy(v)
                    for k, v in dat.items()
                }
            except FileNotFoundError as e:
                return None
            # return None

    層数に対応して書き換える
        ax.bar(
            # [i - 0.3 for i in range(48)],
            [i - 0.3 for i in range(28)],
            avg_ordinary,
            width=0.3,
            color="#7261ab",
            label="Effect of single state on P",
        )
        ax.bar(
            # [i for i in range(48)],
            [i for i in range(28)],
            avg_no_attn,
            width=0.3,
            color="#f3201b",
            label="Effect with Attn severed",
        )
        ax.bar(
            # [i + 0.3 for i in range(48)],
            [i + 0.3 for i in range(28)],
            avg_no_mlp,
            width=0.3,
            color="#20b020",
            label="Effect with MLP severed",
        )

my_research/ROME_server/rome/rome/compute_v.py
    print("romeworkspace/rome/rome/compute_v.py:72")
    # rinna/japanese-gpt-neox-3.6b-instruction-sftは、model.config.n_embdがないので、ベタ打ち
    delta = torch.zeros((2816,), requires_grad=True, device="cuda")
    # delta = torch.zeros((model.config.n_embd,), requires_grad=True, device="cuda")

my_research/ROME_server/rome/rome/rome_main.py
    通常だと、文章の続きを生成するので、空白がない場合は空白を入れる。
    質問形式だと英語でもいらない
        # if request["target_new"]["str"][0] != " ":
        #     # Space required for correct tokenization
        #     request["target_new"]["str"] = " " + request["target_new"]["str"]

my_research/ROME_server/rome/rome/layer_stats.py
    rinnaモデルにない属性だった
    # npos = model.config.n_positions
    npos = 2048

    # maxlen = model.config.n_positions
    maxlen = 2048

my_research/ROME_server/rome/rome/repr_tools.py
    get_words_idxs_in_templatesの切り替え
    まるまる書き換えたので、英語と日本語で書き換える必要あり
    英語用に空白処理がなされている
    文字列操作のロジック：プログラム内で行われる文字列の処理（特に prefixes と suffixes の計算）は、期待される結果に影響を与える可能性があります。例えば、日本語のテキストではスペースが少ないか全くない場合があり、これがトークンのインデックス計算に影響を与える可能性があります。

my_research/ROME_server/rome/experiments/causal_trace.py
    if tokenizer is None:
        assert model_name is not None
        print("experiments/causal_trace 467")
        # tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    if model is None:
        assert model_name is not None
        print("experiments/causal_trace 472")
        # model = AutoModelForCausalLM.from_pretrained(
        #     model_name, low_cpu_mem_usage=low_cpu_mem_usage, torch_dtype=torch_dtype
        # )
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)